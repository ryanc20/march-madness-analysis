import numpy as np
import gym


import mm_data
import mm_env
class QAgent:
    def __init__(self, epsilon, alpha, gamma):
        self.q = {}
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma
        self.env = mm_env.Env()

    def epsilon_greedy_train(self):
        """
        The state will be the matchup (i.e. seed 1 vs seed 16), and the action will
        be the team that won the match. The reward for the (state, action) pair will
        be determined by comparing it to what ACTUALLY happened in that years 
        March Madness tournament. The agent will be trained on each of the years (1985-2016)
        to update potential reward values for the selected action.
        
        In an attempt to account for the randomness of March Madness an epsilon greedy
        approach is used, where if the randomly generated value between 0 and 1 is less
        than the selected value of epsilon, it will select a random action, otherwise
        it will pick the action with the highest reward.
        """
        final_four = []
        championship = []
        conferences = {"W": "East", "X": "Midwest", "Y": "South", "Z": "West"}
        for conf in conferences:
            print("CONFERENCE:", conferences[conf])

            round_1  = self.env.get_round_one()
            print("ROUND1:", round_1)

            first_r_actions = []
            for state in round_1:        
                action = self.env.get_action(self.q, state, self.epsilon)
                first_r_actions.append(action)
             
            round_2 = self.env.get_round_two(first_r_actions)
            print("ROUND2:", round_2)

            second_r_actions = []
            for state in round_2:
                action = self.env.get_action(self.q, state, self.epsilon)
                second_r_actions.append(action)
            
            round_3 = self.env.get_round_three(second_r_actions)
            print("ROUND3:", round_3)

            third_r_actions = []
            for state in round_3:
                action = self.env.get_action(self.q, state, self.epsilon)
                third_r_actions.append(action)

            round_4 = self.env.get_round_four(third_r_actions)
            print("ROUND4:", round_4)

            fourth_r_actions = []
            for state in round_4:
                action = self.env.get_action(self.q, state, self.epsilon)
                fourth_r_actions.append(action)
            
            final_four.append(fourth_r_actions[0])

            print("WINNER:", fourth_r_actions)

        print("FINAL FOUR:", final_four)

        formatted_final_four = []
        formatted_final_four.append("{}-{}".format(final_four[0], final_four[-1]))
        formatted_final_four.append("{}-{}".format(final_four[1], final_four[-2]))

        for state in formatted_final_four:
            action = self.env.get_action(self.q, state, self.epsilon)
            championship.append(action)
        
        print("CHAMPIONSHIP:", championship)
        
        formatted_championship = []
        formatted_championship.append("{}-{}".format(championship[0], championship[-1]))
        for state in formatted_championship:
            action = self.env.get_action(self.q, state, self.epsilon)
            winner = action
        
        print("NCAA CHAMPION:", "['{}']".format(winner))
        print()
            
agent = QAgent(0.25, 0.2, 0.8)
agent.epsilon_greedy_train()
